{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "13PgQ6z1KqQA_g8XgK0Dm-digdvzXgZbd",
      "authorship_tag": "ABX9TyPyvMOMr2fST0cKOSw8Vxfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spiritofdragon/atrium-segmentation/blob/main/atrium_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LvfBpkrAA9FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install celluloid\n",
        "!pip install imgaug\n",
        "!pip install tensorboard\n",
        "!pip install torch\n",
        "!pip intsall torchio\n",
        "!pip install torchmetrics\n",
        "!pip install torchvision\n",
        "!pip install tqdm\n",
        "!pip install pytorch-lightning\n",
        "!pip install opencv-pytorch\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n"
      ],
      "metadata": {
        "id": "AKa_oJkMBHqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEPuv0VhAf1Y"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch # !pip install\n",
        "import pytorch_lightning as pl # !pip install pytorch_lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import imgaug.augmenters as iaa # when run in jupyterlab with pytorch. Try !pip3 install opencv-python-headless==4.5.3.56\n",
        "                                                                            # !pip install imgaug\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq = iaa.Sequential([\n",
        "    iaa.Affine(scale=(0.85, 1.15),\n",
        "              rotate=(-45, 45)),\n",
        "    iaa.ElasticTransformation()\n",
        "])"
      ],
      "metadata": {
        "id": "mzfWEdD_AmY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import imgaug\n",
        "import imgaug.augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage"
      ],
      "metadata": {
        "id": "T0t9uw1FDMcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CardiacDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, augment_params):\n",
        "        self.all_files = self.extract_files(root)\n",
        "        self.augment_params = augment_params\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_files(root):\n",
        "        \"\"\"\n",
        "        Extract the paths to all slices given the root path (ends with train or val)\n",
        "        \"\"\"\n",
        "        files = []\n",
        "        for subject in root.glob(\"*\"):   # Iterate over the subjects\n",
        "            slice_path = subject/\"data\"  # Get the slices for current subject\n",
        "            for slice in slice_path.glob(\"*\"):\n",
        "                files.append(slice)\n",
        "        return files\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def change_img_to_label_path(path):\n",
        "        \"\"\"\n",
        "        Replace data with mask to get the masks\n",
        "        \"\"\"\n",
        "        parts = list(path.parts)\n",
        "        parts[parts.index(\"data\")] = \"mask\"\n",
        "        return Path(*parts)\n",
        "\n",
        "    def augment(self, slice, mask):\n",
        "        \"\"\"\n",
        "        Augments slice and segmentation mask in the exact same way\n",
        "        Note the manual seed initialization\n",
        "        \"\"\"\n",
        "        ###################IMPORTANT###################\n",
        "        # Fix for https://discuss.pytorch.org/t/dataloader-workers-generate-the-same-random-augmentations/28830/2\n",
        "        random_seed = torch.randint(0, 1000000, (1,))[0].item()\n",
        "        imgaug.seed(random_seed)\n",
        "        #####################################################\n",
        "        mask = SegmentationMapsOnImage(mask, mask.shape)\n",
        "        slice_aug, mask_aug = self.augment_params(image=slice, segmentation_maps=mask)\n",
        "        mask_aug = mask_aug.get_arr()\n",
        "        return slice_aug, mask_aug\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of the dataset (length of all files)\n",
        "        \"\"\"\n",
        "        return len(self.all_files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Given an index return the (augmented) slice and corresponding mask\n",
        "        Add another dimension for pytorch\n",
        "        \"\"\"\n",
        "        file_path = self.all_files[idx]\n",
        "        mask_path = self.change_img_to_label_path(file_path)\n",
        "        slice = np.load(file_path).astype(np.float32)\n",
        "        mask = np.load(mask_path)\n",
        "\n",
        "        if self.augment_params:\n",
        "            slice, mask = self.augment(slice, mask)\n",
        "\n",
        "        return np.expand_dims(slice, 0), np.expand_dims(mask, 0)"
      ],
      "metadata": {
        "id": "XkTAmAV2DQBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = Path(\"/content/drive/MyDrive/06-Atrium-Segmentation/Preprocessed/train\")\n",
        "val_path = Path(\"/content/drive/MyDrive/06-Atrium-Segmentation/Preprocessed/val\")\n",
        "\n",
        "train_dataset = CardiacDataset(train_path, seq)\n",
        "val_dataset = CardiacDataset(val_path, None)\n",
        "\n",
        "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
      ],
      "metadata": {
        "id": "PHyq-ELMDEhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "WMOuryQEEC1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Helper Class which implements the intermediate Convolutions\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.step = torch.nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "                                        torch.nn.ReLU(),\n",
        "                                        torch.nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "                                        torch.nn.ReLU())\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.step(X)\n",
        "\n",
        "\n",
        "class UNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a UNet for the Segmentation\n",
        "    We use 3 down- and 3 UpConvolutions and two Convolutions in each step\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Sets up the U-Net Structure\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        ############# DOWN #####################\n",
        "        self.layer1 = DoubleConv(1, 64)\n",
        "        self.layer2 = DoubleConv(64, 128)\n",
        "        self.layer3 = DoubleConv(128, 256)\n",
        "        self.layer4 = DoubleConv(256, 512)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        ############## UP #######################\n",
        "        self.layer5 = DoubleConv(512 + 256, 256)\n",
        "        self.layer6 = DoubleConv(256+128, 128)\n",
        "        self.layer7 = DoubleConv(128+64, 64)\n",
        "        self.layer8 = torch.nn.Conv2d(64, 1, 1)\n",
        "        #########################################\n",
        "\n",
        "        self.maxpool = torch.nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ####### DownConv 1#########\n",
        "        x1 = self.layer1(x)\n",
        "        x1m = self.maxpool(x1)\n",
        "        ###########################\n",
        "\n",
        "        ####### DownConv 2#########\n",
        "        x2 = self.layer2(x1m)\n",
        "        x2m = self.maxpool(x2)\n",
        "        ###########################\n",
        "\n",
        "        ####### DownConv 3#########\n",
        "        x3 = self.layer3(x2m)\n",
        "        x3m = self.maxpool(x3)\n",
        "        ###########################\n",
        "\n",
        "        ##### Intermediate Layer ##\n",
        "        x4 = self.layer4(x3m)\n",
        "        ###########################\n",
        "\n",
        "        ####### UpCONV 1#########\n",
        "        x5 = torch.nn.Upsample(scale_factor=2, mode=\"bilinear\")(x4)  # Upsample with a factor of 2\n",
        "        #x5 = torch.nn.ConvTranspose2d(512, 512, 2, 2)(x4)\n",
        "        x5 = torch.cat([x5, x3], dim=1)  # Skip-Connection\n",
        "        x5 = self.layer5(x5)\n",
        "        ###########################\n",
        "\n",
        "        ####### UpCONV 2#########\n",
        "        x6 = torch.nn.Upsample(scale_factor=2, mode=\"bilinear\")(x5)\n",
        "        #x6 = torch.nn.ConvTranspose2d(256, 256, 2, 2)(x5)\n",
        "        x6 = torch.cat([x6, x2], dim=1)  # Skip-Connection\n",
        "        x6 = self.layer6(x6)\n",
        "        ###########################\n",
        "\n",
        "        ####### UpCONV 3#########\n",
        "        x7 = torch.nn.Upsample(scale_factor=2, mode=\"bilinear\")(x6)\n",
        "        #x7 = torch.nn.ConvTranspose2d(128, 128, 2, 2)(x6)\n",
        "        x7 = torch.cat([x7, x1], dim=1)\n",
        "        x7 = self.layer7(x7)\n",
        "        ###########################\n",
        "\n",
        "        ####### Predicted segmentation#########\n",
        "        ret = self.layer8(x7)\n",
        "        return ret"
      ],
      "metadata": {
        "id": "iyHjbngdEJ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "num_workers = 4\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
      ],
      "metadata": {
        "id": "15kDGtx2DU97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, mask):\n",
        "        pred = torch.flatten(pred)\n",
        "        mask = torch.flatten(mask)\n",
        "\n",
        "        counter = (pred * mask).sum()\n",
        "        denum = pred.sum() + mask.sum() + 1e-8\n",
        "        dice = (2*counter) / denum\n",
        "        return 1 - dice"
      ],
      "metadata": {
        "id": "ENxeETc4EBuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AtriumSegmentation(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = UNet()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
        "        self.loss_fn = DiceLoss()\n",
        "\n",
        "    def forward(self, data):\n",
        "        return torch.sigmoid(self.model(data))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        mri, mask = batch\n",
        "        mask = mask.float()\n",
        "        pred = self(mri)\n",
        "\n",
        "        loss = self.loss_fn(pred, mask)\n",
        "\n",
        "        self.log(\"Train Dice\", loss)\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            self.log_images(mri.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        mri, mask = batch\n",
        "        mask = mask.float()\n",
        "        pred = self(mri)\n",
        "\n",
        "        loss = self.loss_fn(pred, mask)\n",
        "\n",
        "        self.log(\"Val Dice\", loss)\n",
        "\n",
        "        if batch_idx % 2 == 0:\n",
        "            self.log_images(mri.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def log_images(self, mri, pred, mask, name):\n",
        "        pred = pred > 0.5\n",
        "\n",
        "        fig, axis = plt.subplots(1, 2)\n",
        "        axis[0].imshow(mri[0][0], cmap=\"bone\")\n",
        "        mask_ = np.ma.masked_where(mask[0][0] == 0, mask[0][0])\n",
        "        axis[0].imshow(mask_, alpha=0.6)\n",
        "\n",
        "        axis[1].imshow(mri[0][0], cmap=\"bone\")\n",
        "        mask_ = np.ma.masked_where(mask[0][0] == 0, mask[0][0])\n",
        "        axis[1].imshow(mask_, alpha=0.6)\n",
        "\n",
        "        self.logger.experiment.add_figure(name, fig, self.global_step)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return [self.optimizer]"
      ],
      "metadata": {
        "id": "6KLimPo9Dixm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "model = AtriumSegmentation()"
      ],
      "metadata": {
        "id": "VIO_VI-4Hp85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(monitor=\"Val Dice\", save_top_k=10, mode=\"min\")"
      ],
      "metadata": {
        "id": "6IjqfJgIHsab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(accelerator='gpu', devices=1, logger=TensorBoardLogger(save_dir=\"/content/drive/MyDrive/06-Atrium-Segmentation/logs\"), log_every_n_steps=1,\n",
        "                     callbacks=checkpoint_callback,\n",
        "                     max_epochs=75)"
      ],
      "metadata": {
        "id": "XCqFTxxGHusn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "MhSSMl4NHxJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "from tqdm.notebook import tqdm\n",
        "from celluloid import Camera"
      ],
      "metadata": {
        "id": "yus_UKYWIJbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AtriumSegmentation.load_from_checkpoint(\"/content/drive/MyDrive/06-Atrium-Segmentation/logs/lightning_logs/version_5/checkpoints/epoch=71-step=17424.ckpt\")"
      ],
      "metadata": {
        "id": "jqmYNy0UctL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval();\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "_3t69MQhfEX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "labels = []\n",
        "\n",
        "for slice, label in tqdm(val_dataset):\n",
        "    slice = torch.tensor(slice).to(device).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        pred = model(slice)\n",
        "    preds.append(pred.cpu().numpy())\n",
        "    labels.append(label)\n",
        "\n",
        "preds = np.array(preds)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "-Vtvs_uvfFQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1-model.loss_fn(torch.from_numpy(preds), torch.from_numpy(labels))  # two possibilities"
      ],
      "metadata": {
        "id": "UME8ApsdfUYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dice_score = 1-DiceLoss()(torch.from_numpy(preds), torch.from_numpy(labels).unsqueeze(0).float())\n",
        "print(f\"The Val Dice Score is: {dice_score}\")"
      ],
      "metadata": {
        "id": "ATsH_pFXfW1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subject = Path(\"/content/drive/MyDrive/06-Atrium-Segmentation/Task02_Heart/imagesTs/la_002.nii.gz\")\n",
        "subject_mri = nib.load(subject).get_fdata()"
      ],
      "metadata": {
        "id": "RFzq0ZFVpAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions for normalization and standardization\n",
        "def normalize(full_volume):\n",
        "    \"\"\"\n",
        "    Z-Normalization of the whole subject\n",
        "    \"\"\"\n",
        "    mu = full_volume.mean()\n",
        "    std = np.std(full_volume)\n",
        "    normalized = (full_volume - mu) / std\n",
        "    return normalized\n",
        "\n",
        "def standardize(normalized_data):\n",
        "    \"\"\"\n",
        "    Standardize the normalized data into the 0-1 range\n",
        "    \"\"\"\n",
        "    standardized_data = (normalized_data - normalized_data.min()) / (normalized_data.max() - normalized_data.min())\n",
        "    return standardized_data\n"
      ],
      "metadata": {
        "id": "BRVPYhdvpeYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subject_mri = subject_mri[32:-32, 32:-32]\n",
        "standardized_scan = standardize(normalize(subject_mri))"
      ],
      "metadata": {
        "id": "_Rn-0_QdphMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standardized_scan.shape"
      ],
      "metadata": {
        "id": "cJUvgkFgpjuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for i in range(standardized_scan.shape[-1]):\n",
        "    slice = standardized_scan[:,:,i]\n",
        "    with torch.no_grad():\n",
        "        pred = model(torch.tensor(slice).unsqueeze(0).unsqueeze(0).float().to(device))[0][0]\n",
        "        pred = pred > 0.5\n",
        "    preds.append(pred.cpu())"
      ],
      "metadata": {
        "id": "Lqjh5eaHpmJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "camera = Camera(fig)  # create the camera object from celluloid\n",
        "\n",
        "for i in range(standardized_scan.shape[-1]):\n",
        "    plt.imshow(standardized_scan[:,:,i], cmap=\"bone\")\n",
        "    mask = np.ma.masked_where(preds[i]==0, preds[i])\n",
        "    plt.imshow(mask, alpha=0.5, cmap=\"autumn\")\n",
        "\n",
        "    camera.snap()  # Store the current slice\n",
        "animation = camera.animate()  # create the animation"
      ],
      "metadata": {
        "id": "OURfJEyopo43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML(animation.to_html5_video())"
      ],
      "metadata": {
        "id": "lomRboXcpseT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HWd5QTLq_L_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}